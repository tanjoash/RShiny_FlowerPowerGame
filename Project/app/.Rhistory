f.obj_eps <- c(10, 9, 8)
con_1_eps <- c(4, 3, 2)
con_2_eps <- c(3, 2, 2)
con_3_eps <- c(1, 0, 0)
con_4_eps <- c(0, 1, 0)
con_5_eps <- c(0, 0, 1)
con_6_eps <- c(10, 6, 3)
f.con_eps <- rbind(con_1_eps, con_2_eps, con_3_eps, con_4_eps, con_5_eps, con_6_eps)
f.dir_eps <- c("<=", "<=", ">=", ">=", ">=", "<=")
epsilon <- c(0, 480, 960, 1440, 1920, 2400)
#f.rhs_1 <- c(1300, 1000, 0, 0, 0)
objval_eps <- matrix(c(rep(0,6*2)), nrow = 4, byrow = TRUE)
decvar_eps <- matrix(c(rep(0,6*3)), nrow = 4, byrow = TRUE)
for (i in 1:6){
f.rhs_eps <- c(1300, 1000, 0, 0, 0, epsilon[i])
result <- lp("max", f.obj_eps, f.con_eps, f.dir_eps, f.rhs_eps)
objval_eps[i,1] <- result$objval
objval_eps[i,2] <- epsilon[i]
decvar_eps[i,] <- result$solution
}
objval_eps <- matrix(c(rep(0,6*2)), nrow = 6, byrow = TRUE)
decvar_eps <- matrix(c(rep(0,6*3)), nrow = 6, byrow = TRUE)
for (i in 1:6){
f.rhs_eps <- c(1300, 1000, 0, 0, 0, epsilon[i])
result <- lp("max", f.obj_eps, f.con_eps, f.dir_eps, f.rhs_eps)
objval_eps[i,1] <- result$objval
objval_eps[i,2] <- epsilon[i]
decvar_eps[i,] <- result$solution
}
ggplot(data = data.frame(Profit=objval_eps[,1], Pollution=objval_eps[,2]),
mapping = aes(x = Profit, y = Pollution)) +
geom_point(size=3) +
coord_cartesian() +
labs(x="Profit", y="Pollution")
library(randomForest)
library(caret)
library(mlr)
library(data.table)
library(parallelMap)
library(parallel)
parallelStartSocket(cpus = detectCores())
safety <- read.csv("train1_trying.csv")
safety <- subset(safety, select=-c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
head(safety)
seed <- 123
set.seed(seed)
trainingIndex <- createDataPartition(safety$Choice, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
set.seed(seed)
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1], as.factor(trainingSet$Choice),
stepFactor=1.5,improve=0.01, trace=TRUE, plot=FALSE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
set.seed(seed)
model <- randomForest(as.factor(Choice) ~ Price3 + Price2 + income + Price1 + agea + incomea + milesa + nighta + year + yearind + incomeind + miles + milesind + nightind + night + pparkind + ppark + segment + segmentind + region + regionind + BU3, data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
#importance(model)
model
pred <- predict(model, testSet, type="prob")
colnames(pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
loss <- logloss(testSet, as.data.frame(pred))
loss
#set all character variables as factor
fact_col <- colnames(trainingSet)[sapply(trainingSet,is.character)]
for(i in fact_col)
set(trainingSet,j=i,value = factor(trainingSet[[i]]))
for(i in fact_col)
set(testSet,j=i,value = factor(testSet[[i]]))
traintask <- makeClassifTask(data = trainingSet,target = "Choice")
testtask <- makeClassifTask(data = testSet,target = "Choice")
#create learner
bag <- makeLearner("classif.rpart",predict.type = "prob")
bag.lrn <- makeBaggingWrapper(learner = bag,bw.iters = 100,bw.replace = TRUE)
rdesc <- makeResampleDesc("CV",iters=5L)
#Random Forest without Cutoff
rf.lrn <- makeLearner("classif.randomForest")
rf.lrn$par.vals <- list(ntree = 100L,
importance=TRUE)
r <- resample(learner = rf.lrn
,task = traintask
,resampling = rdesc
,measures = list(acc)
,show.info = T,
mtry = best.m,
ntree = 2001)
CVmodel <- train(rf.lrn, traintask)
CVRF <- predict(CVmodel, testtask, type="prob")
CVRF
rfcontrol <- makeTuneControlGrid()
randofor <- makeLearner("classif.randomForest", predict.type = "prob", par.vals = list(ntree = 2001, mtry = best.m))
randofor$par.vals <- list(
importance = TRUE
)
rf_param <- makeParamSet(
makeIntegerParam("ntree",lower = 500, upper = 2100),
makeIntegerParam("mtry", lower = 5, upper = 15),
makeIntegerParam("nodesize", lower = 10, upper = 50)
)
set_cv <- makeResampleDesc("CV",iters = 5L)
rf_tune <- tuneParams(learner = randofor, resampling = set_cv, task = traintask, par.set = rf_param, control = rfcontrol, measures = acc)
library(randomForest)
<<<<<<< Updated upstream
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Predict on the training data
P_train <- predict(rf_model, traindata)
# Convert predicted class labels to numeric
ActualChoice <- traindata[,"Choice"]
PredictedChoice_train <- as.numeric(P_train)
# Create a confusion table for training data
Tabtrain <- table(PredictedChoice_train, ActualChoice)
# Print the confusion table for training data
print(Tabtrain)
# Predict on the test data
P_test <- predict(rf_model, testdata)
# Write the predicted choices to a CSV file
write.csv(P_test, "model2prediction.csv", row.names = FALSE)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Predict on the training data
P_train <- predict(rf_model, traindata, type = "response")
# Convert predicted class labels to numeric
ActualChoice <- traindata[,"Choice"]
PredictedChoice_train <- as.numeric(P_train)
# Create a confusion table for training data
Tabtrain <- table(PredictedChoice_train, ActualChoice)
# Print the confusion table for training data
print(Tabtrain)
# Predict on the test data
P_test <- predict(rf_model, testdata)
# Write the predicted choices to a CSV file
write.csv(P_test, "model2prediction.csv", row.names = FALSE)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Predict on the training data
P_train <- predict(rf_model, traindata, type = "response")
# Convert predicted class labels to numeric
ActualChoice <- traindata[,"Choice"]
PredictedChoice_train <- as.numeric(P_train)
head(PredictedChoice_train)
# Create a confusion table for training data
Tabtrain <- table(PredictedChoice_train, ActualChoice)
# Print the confusion table for training data
print(Tabtrain)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- row.names(var_imp)[1:20]
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
# Fit the Random Forest model for classification
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- row.names(var_imp)[1:20]
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- row.names(var_imp)[1:20]
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- row.names(var_imp)[1:20]
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
var_imp
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
head(var_imp)
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
print(var_imp)
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
print(var_imp)
head(var_imp)
str(var_imp)
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- importance(rf_model)
# Convert to dataframe and add predictor names
var_imp_df <- data.frame(Predictor = names(var_imp), Importance = var_imp)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- importance(rf_model)
var_imp <- setNames(var_imp, names(var_imp))
# Convert to dataframe and add predictor names
var_imp_df <- data.frame(Predictor = names(var_imp), Importance = var_imp)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- importance(rf_model)
predictor_names <- colnames(traindata)[!colnames(traindata) %in% c("Choice", "No", "Case")]
# Convert to dataframe and add predictor names
var_imp_df <- data.frame(Predictor = names(var_imp), Importance = var_imp)
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
=======
library(caret)
library(mlr)
library(data.table)
library(parallelMap)
library(parallel)
parallelStartSocket(cpus = detectCores())
safety <- read.csv("train1_trying.csv")
safety <- subset(safety, select=-c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
head(safety)
seed <- 123
set.seed(seed)
trainingIndex <- createDataPartition(safety$Choice, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
set.seed(seed)
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1], as.factor(trainingSet$Choice),
stepFactor=1.5,improve=0.01, trace=TRUE, plot=FALSE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
set.seed(seed)
model <- randomForest(as.factor(Choice) ~ Price3 + Price2 + income + Price1 + agea + incomea + milesa + nighta + year + yearind + incomeind + miles + milesind + nightind + night + pparkind + ppark + segment + segmentind + region + regionind + BU3, data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
#importance(model)
model
pred <- predict(model, testSet, type="prob")
colnames(pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
loss <- logloss(testSet, as.data.frame(pred))
loss
#set all character variables as factor
fact_col <- colnames(trainingSet)[sapply(trainingSet,is.character)]
for(i in fact_col)
set(trainingSet,j=i,value = factor(trainingSet[[i]]))
for(i in fact_col)
set(testSet,j=i,value = factor(testSet[[i]]))
traintask <- makeClassifTask(data = trainingSet,target = "Choice")
testtask <- makeClassifTask(data = testSet,target = "Choice")
rdesc <- makeResampleDesc("CV",iters=5L)
#Random Forest without Cutoff
rf.lrn <- makeLearner("classif.randomForest", predict.type = "prob")
rf.lrn$par.vals <- list(ntree = 100L,
importance=TRUE)
r <- resample(learner = rf.lrn
,task = traintask
,resampling = rdesc
,measures = list(acc)
,show.info = T,
mtry = best.m,
ntree = 2001)
CVmodel <- train(rf.lrn, traintask)
CVRF <- predict(CVmodel, testtask, type="prob")
CVRF
CVRF <- CVRF[3:6]
CVRF
CVRF <- predict(CVmodel, testtask, type="prob")
CVRF
CVRF <- CVRF[,3:6]
CVRF <- CVRF[,3:4]
View(CVRF)
CVRF <- CVRF$data
CVRF
CVRF <- CVRF$data[,3:6]
CVRF
CVRF <- predict(CVmodel, testtask, type="prob")
CVRF
CVRF <- CVRF$data[,3:6]
CVRF
colnames(CVRF) <- c("Ch1", "Ch2", "Ch3", "Ch4")
colnames(CVRF) <- c("Ch1", "Ch2", "Ch3", "Ch4")
CVloss <- logloss(testSet, CVRF)
colnames(CVRF) <- c("Ch1", "Ch2", "Ch3", "Ch4")
CVloss <- logloss(testSet, CVRF)
cvloss
colnames(CVRF) <- c("Ch1", "Ch2", "Ch3", "Ch4")
CVloss <- logloss(testSet, CVRF)
CVloss
getNum <- read.csv("./test1_trying.csv")
test <- subset(getNum, select = -c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
#fact_col <- colnames(test)[sapply(test,is.character)]
for(i in fact_col)
set(test,j=i,value = factor(test[[i]]))
fact_col <- colnames(test)[sapply(test,is.character)]
for(i in fact_col)
set(test,j=i,value = factor(test[[i]]))
getNum <- read.csv("./test1_trying.csv")
test <- subset(getNum, select = -c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
fact_col <- colnames(test)[sapply(test,is.character)]
for(i in fact_col)
set(test,j=i,value = factor(test[[i]]))
shiny::runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
View(eodOrdered)
shiny::runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project/app')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
shiny::runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
>>>>>>> Stashed changes
shiny::runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/tetar/Desktop/RShiny_FlowerPowerGame/Project')
