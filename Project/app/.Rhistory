<<<<<<< Updated upstream
testSet$Choice_pred <- pred
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
testSet$ch1 <- ifelse(test_set$Choice == 1, 1, 0)
testSet$ch1 <- ifelse(testSet$Choice == 1, 1, 0)
testSet$ch2 <- ifelse(testSet$Choice == 2, 1, 0)
testSet$ch3 <- ifelse(testSet$Choice == 3, 1, 0)
testSet$ch4 <- ifelse(testSet$Choice ==4, 1, 0)
logloss <- -mean(log(pred[cbind(1:nrow(testSet), testSet$Choice + 1)]))
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
pred
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 100)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
logLoss <- function(pred, actual){
-mean(actual * log(pred) + (1 - actual) * log(1 - pred))
}
logLoss(result, testSet$Choice)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
logLoss <- function(pred, actual){
-mean(actual * log(pred) + (1 - actual) * log(1 - pred))
}
logLoss(testSet$Choice_pred, testSet$Choice)
n <- length(testSet$Choice)
m <- max(testSet$Choice)
# Step 1: Create an empty probability matrix
probability_matrix <- matrix(0, nrow = n, ncol = m)
# Step 2: Assign the predicted probabilities
for (i in 1:n) {
probability_matrix[i, testSet$Choice_pred[i]] <- 1
=======
>>>>>>> Stashed changes
}
# Now, you have a probability matrix where each row represents the predicted probabilities for a sample.
# Step 3: Calculate log loss using the probability matrix and true class labels
log_loss <- -(1/n) * sum(log(probability_matrix[cbind(1:n, testSet$Choice)]))
n <- length(testSet$Choice)
m <- max(testSet$Choice)
# Step 1: Create an empty probability matrix
probability_matrix <- matrix(0, nrow = n, ncol = m)
# Step 2: Assign the predicted probabilities
for (i in 1:n) {
probability_matrix[i, testSet$Choice_pred[i]] <- 1
}
# Now, you have a probability matrix where each row represents the predicted probabilities for a sample.
# Step 3: Calculate log loss using the probability matrix and true class labels
log_loss <- -(1/n) * sum(log(probability_matrix[cbind(1:n, testSet$Choice)]))
log_loss
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 100)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
n <- length(testSet$Choice)
m <- max(testSet$Choice)
# Step 1: Create an empty probability matrix
probability_matrix <- matrix(0, nrow = n, ncol = m)
# Step 2: Assign the predicted probabilities
for (i in 1:n) {
probability_matrix[i, testSet$Choice_pred[i]] <- 1
}
# Now, you have a probability matrix where each row represents the predicted probabilities for a sample.
# Step 3: Calculate log loss using the probability matrix and true class labels
log_loss <- -(1/n) * sum(log(probability_matrix[cbind(1:n, testSet$Choice)]))
log_loss
n <- length(testSet$Choice)
m <- max(testSet$Choice)
# Step 1: Create an empty probability matrix
probability_matrix <- matrix(0, nrow = n, ncol = m)
# Step 2: Assign the predicted probabilities
for (i in 1:n) {
probability_matrix[i, testSet$Choice_pred[i]] <- 1
}
# Now, you have a probability matrix where each row represents the predicted probabilities for a sample.
# Step 3: Calculate log loss using the probability matrix and true class labels
log_loss <- -(1/n) * sum(log(probability_matrix[cbind(1:n, testSet$Choice)]))
probability_matrix
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 100)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
log_loss <- function(actual_values, predicted_values) {
num_samples <- length(actual_values)
num_classes <- 4  # Since the values range from 1 to 4
total_loss <- 0
for (i in 1:num_samples) {
actual_val <- actual_values[i]
predicted_val <- predicted_values[i]
# One-hot encode the actual value
one_hot_actual <- rep(0, num_classes)
one_hot_actual[actual_val] <- 1
# Calculate the log loss for this sample
total_loss <- total_loss + sum(one_hot_actual * log(predicted_val) + (1 - one_hot_actual) * log(1 - predicted_val))
}
logloss <- -total_loss / num_samples
return(logloss)
}
# Example usage:
result_logloss <- log_loss(testSet$Choice, testSet$Choice_pred)
print(paste("Log Loss:", result_logloss))
log_loss <- function(actual, predicted) {
n <- length(actual)
actual <- as.integer(actual) # Ensure actual values are integers
predicted <- as.integer(predicted) # Ensure predicted values are integers
# Convert actual and predicted values to one-hot encoding
actual_one_hot <- matrix(0, nrow = n, ncol = max(actual))
predicted_one_hot <- matrix(0, nrow = n, ncol = max(predicted))
actual_one_hot[cbind(1:n, actual)] <- 1
predicted_one_hot[cbind(1:n, predicted)] <- 1
# Calculate log loss
epsilon <- 1e-15 # Small value to avoid division by zero
log_loss <- -sum(actual_one_hot * log(pmax(predicted_one_hot, epsilon))) / n
return(log_loss)
}
loss <- log_loss(testSet$Choice, testeSet$Choice_pred)
log_loss <- function(actual, predicted) {
n <- length(actual)
actual <- as.integer(actual) # Ensure actual values are integers
predicted <- as.integer(predicted) # Ensure predicted values are integers
# Convert actual and predicted values to one-hot encoding
actual_one_hot <- matrix(0, nrow = n, ncol = max(actual))
predicted_one_hot <- matrix(0, nrow = n, ncol = max(predicted))
actual_one_hot[cbind(1:n, actual)] <- 1
predicted_one_hot[cbind(1:n, predicted)] <- 1
# Calculate log loss
epsilon <- 1e-15 # Small value to avoid division by zero
log_loss <- -sum(actual_one_hot * log(pmax(predicted_one_hot, epsilon))) / n
return(log_loss)
}
loss <- log_loss(testSet$Choice, testSet$Choice_pred)
print(loss)
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 500)
mtry <- tuneRF(trainingSet[-1],trainingSet$Choice, ntreeTry=500,
stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
trainingSet[1]
trainingSet[-1]
trainingSet[1:5]
trainingSet[1:ncol(trainingSet)-1]
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 500)
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1],trainingSet$Choice, ntreeTry=500,
stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
log_loss <- function(actual, predicted) {
n <- length(actual)
actual <- as.integer(actual) # Ensure actual values are integers
predicted <- as.integer(predicted) # Ensure predicted values are integers
# Convert actual and predicted values to one-hot encoding
actual_one_hot <- matrix(0, nrow = n, ncol = max(actual))
predicted_one_hot <- matrix(0, nrow = n, ncol = max(predicted))
actual_one_hot[cbind(1:n, actual)] <- 1
predicted_one_hot[cbind(1:n, predicted)] <- 1
# Calculate log loss
epsilon <- 1e-15 # Small value to avoid division by zero
log_loss <- -sum(actual_one_hot * log(pmax(predicted_one_hot, epsilon))) / n
return(log_loss)
}
loss <- log_loss(testSet$Choice, testSet$Choice_pred)
print(loss)
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 500)
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1],trainingSet$Choice, ntreeTry=500,
stepFactor=1.5,improve=0.001, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
set.seed(seed)
rf <-randomForest(Choice~.,data=trainingSet, mtry=best.m, importance=TRUE,ntree=500)
print(rf)
#Evaluate variable importance
importance(rf)
varImpPlot(rf)
pred <- predict(rf, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
log_loss <- function(actual, predicted) {
n <- length(actual)
actual <- as.integer(actual) # Ensure actual values are integers
predicted <- as.integer(predicted) # Ensure predicted values are integers
# Convert actual and predicted values to one-hot encoding
actual_one_hot <- matrix(0, nrow = n, ncol = max(actual))
predicted_one_hot <- matrix(0, nrow = n, ncol = max(predicted))
actual_one_hot[cbind(1:n, actual)] <- 1
predicted_one_hot[cbind(1:n, predicted)] <- 1
# Calculate log loss
epsilon <- 1e-15 # Small value to avoid division by zero
log_loss <- -sum(actual_one_hot * log(pmax(predicted_one_hot, epsilon))) / n
return(log_loss)
}
loss <- log_loss(testSet$Choice, testSet$Choice_pred)
print(loss)
rm(list=ls())
library(randomForest)
library(caret)
safety <- read.csv("train1.csv")
# Libraries
rm(list=ls())
library(randomForest)
library(caret)
# Read csv
safety <- read.csv("train1_preprocessed.csv")
# Remove unnecessary columns
safety <- subset(safety, select=-c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
# Seed number
seed <- 123
# Use caret library to create partition with 80:20 split
set.seed(seed)
trainingIndex <- createDataPartition(safety$Choice, p = 0.8, list = FALSE)
# Get train and test set
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
# Use tuneRF function to find optimal mtry
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1], as.factor(trainingSet$Choice),
stepFactor=1.5,improve=0.01, trace=TRUE, plot=FALSE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
# Model
set.seed(seed)
model <- randomForest(as.factor(Choice) ~ ., data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
# Model
model <- randomForest(as.factor(Choice) ~ ., data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
# Results
#importance(model)
model
# Predict
pred <- predict(model, testSet, type="prob")
# Change col names to Ch1, Ch2, Ch3, Ch4 to calculate logloss using function
colnames(pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
# Function calculating logloss
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
# Calculate logloss
loss <- logloss(testSet, as.data.frame(pred))
loss #best result: 1.145054
# Libraries
rm(list=ls())
library(randomForest)
<<<<<<< Updated upstream
library(caret)
# Read csv
safety <- read.csv("train1_preprocessed.csv")
# Remove unnecessary columns
safety <- subset(safety, select=-c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
# Seed number
seed <- 123
# Use caret library to create partition with 80:20 split
set.seed(seed)
trainingIndex <- createDataPartition(safety$Choice, p = 0.8, list = FALSE)
# Get train and test set
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
# Use tuneRF function to find optimal mtry
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1], as.factor(trainingSet$Choice),
stepFactor=1.5,improve=0.01, trace=TRUE, plot=FALSE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1] #13
# Model
model <- randomForest(as.factor(Choice) ~ ., data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
# Results
#importance(model)
model
# Predict
pred <- predict(model, testSet, type="prob")
# Change col names to Ch1, Ch2, Ch3, Ch4 to calculate logloss using function
colnames(pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
# Function calculating logloss
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
# Calculate logloss
loss <- logloss(testSet, as.data.frame(pred))
loss #best result: 1.145054
# Model
set.seed(seed)
model <- randomForest(as.factor(Choice) ~ ., data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
# Results
#importance(model)
model
# Predict
pred <- predict(model, testSet, type="prob")
# Change col names to Ch1, Ch2, Ch3, Ch4 to calculate logloss using function
colnames(pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
# Function calculating logloss
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
# Calculate logloss
loss <- logloss(testSet, as.data.frame(pred))
loss #best result: 1.145054
# Read test csv
getNum <- read.csv("./test1_preprocessed.csv")
# Remove unnecessary columns
test <- subset(getNum, select = -c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
# Test prediction
final_predict <- predict(model, test, type="prob")
# Change col names to Ch1, Ch2, Ch3, Ch4 to fit submission format
colnames(final_predict) <- c("Ch1","Ch2","Ch3","Ch4")
# Change to dataframe
final_predict_df <- as.data.frame(final_predict)
# Add No column
final_predict_df$No <- getNum$No
# Change column orientation to fit submission format
final_predict_df <- final_predict_df[c("No","Ch1","Ch2","Ch3","Ch4")]
# Export
write.csv(final_predict_df, file = "../Output/RandomForest_best.csv", row.names = FALSE)
shiny::runApp('C:/Users/Joash Tan/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/Joash Tan/Desktop/RShiny_FlowerPowerGame/Project')
=======
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Predict on the training data
P_train <- predict(rf_model, traindata)
# Convert predicted class labels to numeric
PredictedChoice_train <- apply(P_train,1,which.max)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Predict on the training data
P_train <- predict(rf_model, traindata)
# Convert predicted class labels to numeric
PredictedChoice_train <- as.numeric(P_train)
# Create a confusion table for training data
Tabtrain <- table(PredictedChoice_train, traindata$Choice)
# Print the confusion table for training data
print(Tabtrain)
# Predict on the test data
P_test <- predict(rf_model, testdata)
# Write the predicted choices to a CSV file
write.csv(P_test, "model2prediction.csv", row.names = FALSE)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Predict on the training data
P_train <- predict(rf_model, traindata)
# Convert predicted class labels to numeric
ActualChoice <- traindata[,"Choice"]
PredictedChoice_train <- as.numeric(P_train)
# Create a confusion table for training data
Tabtrain <- table(PredictedChoice_train, ActualChoice)
# Print the confusion table for training data
print(Tabtrain)
# Predict on the test data
P_test <- predict(rf_model, testdata)
# Write the predicted choices to a CSV file
write.csv(P_test, "model2prediction.csv", row.names = FALSE)
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
str(traindata)
summary(traindata)
head(traindata)
?apply
traindata$Choice<- max.col(traindata[110:113])
table(traindata$Choice)
which(colnames(traindata)=="CC1")
which(colnames(traindata)=="Price4")
library(mlogit)
S <- dfidx(traindata, shape="wide", choice="Choice", sep="", varying = c(4:83), idx = c("No", "Case"))
head(S)
str(S)
M <- mlogit(Choice~CC+GN+NS+BU+FA+LD+BZ+FC+FP+RP+PP+KA+SC+TS+NV+MA+LB+AF+HU+Price-1, data=S)
summary(M)
M$logLik
ActualChoice <- traindata[,"Choice"]
P <- predict(M, newdata=S)
PredictedChoice <- apply(P,1,which.max)
Tabtrain=table(PredictedChoice, ActualChoice)
Tabtrain
acc<- sum(diag(Tabtrain))/sum (Tabtrain)
acc
# LOG LOSS
sum((traindata[,110:113]*log(P))) / -21565
### sorry ignore this block - Melvin
# Logloss after 1 hot encoding
#my_data <-max.col(P)
#write.csv(P, "prediction2.csv", row.names=FALSE)
#data <- data.frame(Category = my_data)
#encoded_data <- model.matrix(~ 0 + Category, data = data)
#onehotdata <- read.csv("prediction2.csv")
#sum((traindata[,110:113]*onehotdata)) / -21565
# adding random number to choice (just to dfidx)
testdata$Choice<- sample(1:4,nrow(testdata), replace=TRUE)
test <- dfidx(testdata, shape="wide", choice="Choice", sep="", varying = c(4:83), idx = c("No", "Case"))
#submissions want the P
P <- predict(M, newdata=test)
write.csv(P, "model1prediction.csv", row.names=FALSE)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Predict on the training data
P_train <- predict(rf_model, traindata)
# Convert predicted class labels to numeric
ActualChoice <- traindata[,"Choice"]
PredictedChoice_train <- as.numeric(P_train)
# Create a confusion table for training data
Tabtrain <- table(PredictedChoice_train, ActualChoice)
# Print the confusion table for training data
print(Tabtrain)
# Predict on the test data
P_test <- predict(rf_model, testdata)
# Write the predicted choices to a CSV file
write.csv(P_test, "model2prediction.csv", row.names = FALSE)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Predict on the training data
P_train <- predict(rf_model, traindata, type = "response")
# Convert predicted class labels to numeric
ActualChoice <- traindata[,"Choice"]
PredictedChoice_train <- as.numeric(P_train)
# Create a confusion table for training data
Tabtrain <- table(PredictedChoice_train, ActualChoice)
# Print the confusion table for training data
print(Tabtrain)
# Predict on the test data
P_test <- predict(rf_model, testdata)
# Write the predicted choices to a CSV file
write.csv(P_test, "model2prediction.csv", row.names = FALSE)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Predict on the training data
P_train <- predict(rf_model, traindata, type = "response")
# Convert predicted class labels to numeric
ActualChoice <- traindata[,"Choice"]
PredictedChoice_train <- as.numeric(P_train)
head(PredictedChoice_train)
# Create a confusion table for training data
Tabtrain <- table(PredictedChoice_train, ActualChoice)
# Print the confusion table for training data
print(Tabtrain)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- row.names(var_imp)[1:20]
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
# Fit the Random Forest model for classification
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- row.names(var_imp)[1:20]
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- row.names(var_imp)[1:20]
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
library(randomForest)
# Read the data
traindata <- read.csv("train1.csv")
testdata <- read.csv("test1.csv")
# Convert the "Choice" variable to a factor for classification
traindata$Choice<- max.col(traindata[110:113])
traindata$Choice <- as.factor(traindata$Choice)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- row.names(var_imp)[1:20]
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
var_imp
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
head(var_imp)
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
print(var_imp)
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- as.data.frame(sort(importance(rf_model), decreasing = TRUE))
print(var_imp)
head(var_imp)
str(var_imp)
top_20_predictors <- rownames(head(var_imp, 20))
top_20_predictors
# Modify the formula for the top 20 predictors
formula_str <- paste("Choice ~", paste(top_20_predictors, collapse = " + "))
print(formula_str)
# Refit the Random Forest model with the top 20 predictors
rf_model <- randomForest(as.formula(formula_str), data = traindata, ntree = 100)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- importance(rf_model)
# Convert to dataframe and add predictor names
var_imp_df <- data.frame(Predictor = names(var_imp), Importance = var_imp)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- importance(rf_model)
var_imp <- setNames(var_imp, names(var_imp))
# Convert to dataframe and add predictor names
var_imp_df <- data.frame(Predictor = names(var_imp), Importance = var_imp)
# Fit the Random Forest model for classification
rf_model <- randomForest(Choice ~ . - No - Case, data = traindata, ntree = 100)
# Feature selection: Get the top 20 most important predictors
var_imp <- importance(rf_model)
predictor_names <- colnames(traindata)[!colnames(traindata) %in% c("Choice", "No", "Case")]
# Convert to dataframe and add predictor names
var_imp_df <- data.frame(Predictor = names(var_imp), Importance = var_imp)
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
shiny::runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project/app')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
runApp('GitHub/RShiny_FlowerPowerGame/Project')
>>>>>>> Stashed changes
