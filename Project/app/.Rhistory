testSet$Choice_pred <- pred
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
testSet$ch1 <- ifelse(test_set$Choice == 1, 1, 0)
testSet$ch1 <- ifelse(testSet$Choice == 1, 1, 0)
testSet$ch2 <- ifelse(testSet$Choice == 2, 1, 0)
testSet$ch3 <- ifelse(testSet$Choice == 3, 1, 0)
testSet$ch4 <- ifelse(testSet$Choice ==4, 1, 0)
logloss <- -mean(log(pred[cbind(1:nrow(testSet), testSet$Choice + 1)]))
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
pred
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 100)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
logLoss <- function(pred, actual){
-mean(actual * log(pred) + (1 - actual) * log(1 - pred))
}
logLoss(result, testSet$Choice)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
logLoss <- function(pred, actual){
-mean(actual * log(pred) + (1 - actual) * log(1 - pred))
}
logLoss(testSet$Choice_pred, testSet$Choice)
n <- length(testSet$Choice)
m <- max(testSet$Choice)
# Step 1: Create an empty probability matrix
probability_matrix <- matrix(0, nrow = n, ncol = m)
# Step 2: Assign the predicted probabilities
for (i in 1:n) {
probability_matrix[i, testSet$Choice_pred[i]] <- 1
}
# Now, you have a probability matrix where each row represents the predicted probabilities for a sample.
# Step 3: Calculate log loss using the probability matrix and true class labels
log_loss <- -(1/n) * sum(log(probability_matrix[cbind(1:n, testSet$Choice)]))
n <- length(testSet$Choice)
m <- max(testSet$Choice)
# Step 1: Create an empty probability matrix
probability_matrix <- matrix(0, nrow = n, ncol = m)
# Step 2: Assign the predicted probabilities
for (i in 1:n) {
probability_matrix[i, testSet$Choice_pred[i]] <- 1
}
# Now, you have a probability matrix where each row represents the predicted probabilities for a sample.
# Step 3: Calculate log loss using the probability matrix and true class labels
log_loss <- -(1/n) * sum(log(probability_matrix[cbind(1:n, testSet$Choice)]))
log_loss
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 100)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
n <- length(testSet$Choice)
m <- max(testSet$Choice)
# Step 1: Create an empty probability matrix
probability_matrix <- matrix(0, nrow = n, ncol = m)
# Step 2: Assign the predicted probabilities
for (i in 1:n) {
probability_matrix[i, testSet$Choice_pred[i]] <- 1
}
# Now, you have a probability matrix where each row represents the predicted probabilities for a sample.
# Step 3: Calculate log loss using the probability matrix and true class labels
log_loss <- -(1/n) * sum(log(probability_matrix[cbind(1:n, testSet$Choice)]))
log_loss
n <- length(testSet$Choice)
m <- max(testSet$Choice)
# Step 1: Create an empty probability matrix
probability_matrix <- matrix(0, nrow = n, ncol = m)
# Step 2: Assign the predicted probabilities
for (i in 1:n) {
probability_matrix[i, testSet$Choice_pred[i]] <- 1
}
# Now, you have a probability matrix where each row represents the predicted probabilities for a sample.
# Step 3: Calculate log loss using the probability matrix and true class labels
log_loss <- -(1/n) * sum(log(probability_matrix[cbind(1:n, testSet$Choice)]))
probability_matrix
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 100)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
log_loss <- function(actual_values, predicted_values) {
num_samples <- length(actual_values)
num_classes <- 4  # Since the values range from 1 to 4
total_loss <- 0
for (i in 1:num_samples) {
actual_val <- actual_values[i]
predicted_val <- predicted_values[i]
# One-hot encode the actual value
one_hot_actual <- rep(0, num_classes)
one_hot_actual[actual_val] <- 1
# Calculate the log loss for this sample
total_loss <- total_loss + sum(one_hot_actual * log(predicted_val) + (1 - one_hot_actual) * log(1 - predicted_val))
}
logloss <- -total_loss / num_samples
return(logloss)
}
# Example usage:
result_logloss <- log_loss(testSet$Choice, testSet$Choice_pred)
print(paste("Log Loss:", result_logloss))
log_loss <- function(actual, predicted) {
n <- length(actual)
actual <- as.integer(actual) # Ensure actual values are integers
predicted <- as.integer(predicted) # Ensure predicted values are integers
# Convert actual and predicted values to one-hot encoding
actual_one_hot <- matrix(0, nrow = n, ncol = max(actual))
predicted_one_hot <- matrix(0, nrow = n, ncol = max(predicted))
actual_one_hot[cbind(1:n, actual)] <- 1
predicted_one_hot[cbind(1:n, predicted)] <- 1
# Calculate log loss
epsilon <- 1e-15 # Small value to avoid division by zero
log_loss <- -sum(actual_one_hot * log(pmax(predicted_one_hot, epsilon))) / n
return(log_loss)
}
loss <- log_loss(testSet$Choice, testeSet$Choice_pred)
log_loss <- function(actual, predicted) {
n <- length(actual)
actual <- as.integer(actual) # Ensure actual values are integers
predicted <- as.integer(predicted) # Ensure predicted values are integers
# Convert actual and predicted values to one-hot encoding
actual_one_hot <- matrix(0, nrow = n, ncol = max(actual))
predicted_one_hot <- matrix(0, nrow = n, ncol = max(predicted))
actual_one_hot[cbind(1:n, actual)] <- 1
predicted_one_hot[cbind(1:n, predicted)] <- 1
# Calculate log loss
epsilon <- 1e-15 # Small value to avoid division by zero
log_loss <- -sum(actual_one_hot * log(pmax(predicted_one_hot, epsilon))) / n
return(log_loss)
}
loss <- log_loss(testSet$Choice, testSet$Choice_pred)
print(loss)
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 500)
mtry <- tuneRF(trainingSet[-1],trainingSet$Choice, ntreeTry=500,
stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
trainingSet[1]
trainingSet[-1]
trainingSet[1:5]
trainingSet[1:ncol(trainingSet)-1]
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 500)
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1],trainingSet$Choice, ntreeTry=500,
stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
pred <- predict(model, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
log_loss <- function(actual, predicted) {
n <- length(actual)
actual <- as.integer(actual) # Ensure actual values are integers
predicted <- as.integer(predicted) # Ensure predicted values are integers
# Convert actual and predicted values to one-hot encoding
actual_one_hot <- matrix(0, nrow = n, ncol = max(actual))
predicted_one_hot <- matrix(0, nrow = n, ncol = max(predicted))
actual_one_hot[cbind(1:n, actual)] <- 1
predicted_one_hot[cbind(1:n, predicted)] <- 1
# Calculate log loss
epsilon <- 1e-15 # Small value to avoid division by zero
log_loss <- -sum(actual_one_hot * log(pmax(predicted_one_hot, epsilon))) / n
return(log_loss)
}
loss <- log_loss(testSet$Choice, testSet$Choice_pred)
print(loss)
rm(list=ls())
# Execute our custom script for loading packages
source("usePackages.R")
# Name of the packages
pkgnames <- c("caret","randomForest", "stats", "dplyr")
# Use our custom load function
loadPkgs(pkgnames)
# Set seed
seed <- 41
safety <- read.csv("./train1_preprocessed.csv")
test <- read.csv("./test1_preprocessed.csv")
#select_cols <- c("segmentind", "yearind", "milesind", "milesa", "nightind", "nighta", #"pparkind","genderind", "ageind", "agea","educind", "regionind", "Urbind", "incomeind", #"incomea")
#for (i in select_cols) {
#  result <- table(safety[[i]])
#  print(paste("Frequency table for", i))
#  print(result) # Check for 1s
#  print("-------------------------------")
#}
safety <- subset(safety, select = -c(Case, No, Task, Ch1, Ch2, Ch3, Ch4, CC4, BU4, RP4, LD4, BZ4, FC4, PP4, KA4, SC4, TS4, NV4, Price4))
set.seed(seed)
trainingIndex <- createDataPartition(y = safety$Choice, times = 1, p = 0.8, list = FALSE)
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
trainingSet$Choice <- as.factor(trainingSet$Choice)
set.seed(seed)
model <- randomForest(Choice ~ .,
data = trainingSet,
ntree = 500)
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1],trainingSet$Choice, ntreeTry=500,
stepFactor=1.5,improve=0.001, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
set.seed(seed)
rf <-randomForest(Choice~.,data=trainingSet, mtry=best.m, importance=TRUE,ntree=500)
print(rf)
#Evaluate variable importance
importance(rf)
varImpPlot(rf)
pred <- predict(rf, testSet, type="response")
testSet$Choice_pred <- pred
testSet$Choice_pred <- as.numeric(testSet$Choice_pred)
cfn_table <- table(testSet$Choice, testSet$Choice_pred)
cfn_table
log_loss <- function(actual, predicted) {
n <- length(actual)
actual <- as.integer(actual) # Ensure actual values are integers
predicted <- as.integer(predicted) # Ensure predicted values are integers
# Convert actual and predicted values to one-hot encoding
actual_one_hot <- matrix(0, nrow = n, ncol = max(actual))
predicted_one_hot <- matrix(0, nrow = n, ncol = max(predicted))
actual_one_hot[cbind(1:n, actual)] <- 1
predicted_one_hot[cbind(1:n, predicted)] <- 1
# Calculate log loss
epsilon <- 1e-15 # Small value to avoid division by zero
log_loss <- -sum(actual_one_hot * log(pmax(predicted_one_hot, epsilon))) / n
return(log_loss)
}
loss <- log_loss(testSet$Choice, testSet$Choice_pred)
print(loss)
rm(list=ls())
library(randomForest)
library(caret)
safety <- read.csv("train1.csv")
# Libraries
rm(list=ls())
library(randomForest)
library(caret)
# Read csv
safety <- read.csv("train1_preprocessed.csv")
# Remove unnecessary columns
safety <- subset(safety, select=-c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
# Seed number
seed <- 123
# Use caret library to create partition with 80:20 split
set.seed(seed)
trainingIndex <- createDataPartition(safety$Choice, p = 0.8, list = FALSE)
# Get train and test set
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
# Use tuneRF function to find optimal mtry
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1], as.factor(trainingSet$Choice),
stepFactor=1.5,improve=0.01, trace=TRUE, plot=FALSE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
# Model
set.seed(seed)
model <- randomForest(as.factor(Choice) ~ ., data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
# Model
model <- randomForest(as.factor(Choice) ~ ., data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
# Results
#importance(model)
model
# Predict
pred <- predict(model, testSet, type="prob")
# Change col names to Ch1, Ch2, Ch3, Ch4 to calculate logloss using function
colnames(pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
# Function calculating logloss
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
# Calculate logloss
loss <- logloss(testSet, as.data.frame(pred))
loss #best result: 1.145054
# Libraries
rm(list=ls())
library(randomForest)
library(caret)
# Read csv
safety <- read.csv("train1_preprocessed.csv")
# Remove unnecessary columns
safety <- subset(safety, select=-c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
# Seed number
seed <- 123
# Use caret library to create partition with 80:20 split
set.seed(seed)
trainingIndex <- createDataPartition(safety$Choice, p = 0.8, list = FALSE)
# Get train and test set
trainingSet <- safety[trainingIndex,]
testSet <- safety[-trainingIndex,]
# Use tuneRF function to find optimal mtry
mtry <- tuneRF(trainingSet[1:ncol(trainingSet)-1], as.factor(trainingSet$Choice),
stepFactor=1.5,improve=0.01, trace=TRUE, plot=FALSE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1] #13
# Model
model <- randomForest(as.factor(Choice) ~ ., data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
# Results
#importance(model)
model
# Predict
pred <- predict(model, testSet, type="prob")
# Change col names to Ch1, Ch2, Ch3, Ch4 to calculate logloss using function
colnames(pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
# Function calculating logloss
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
# Calculate logloss
loss <- logloss(testSet, as.data.frame(pred))
loss #best result: 1.145054
# Model
set.seed(seed)
model <- randomForest(as.factor(Choice) ~ ., data = trainingSet, mtry=best.m, importance=TRUE, ntree = 2001)
# Results
#importance(model)
model
# Predict
pred <- predict(model, testSet, type="prob")
# Change col names to Ch1, Ch2, Ch3, Ch4 to calculate logloss using function
colnames(pred) <- c("Ch1", "Ch2", "Ch3", "Ch4")
# Function calculating logloss
logloss <- function(test_set, testpredict_df) {
# Create one-hot encoding for each choice on-the-fly
Ch1 <- as.integer(test_set$Choice == 1)
Ch2 <- as.integer(test_set$Choice == 2)
Ch3 <- as.integer(test_set$Choice == 3)
Ch4 <- as.integer(test_set$Choice == 4)
# Calculate logloss using these one-hot encoded variables
result <- -1/nrow(test_set) * sum(Ch1 * log(testpredict_df$Ch1+.Machine$double.eps) +
Ch2 * log(testpredict_df$Ch2+.Machine$double.eps) +
Ch3 * log(testpredict_df$Ch3+.Machine$double.eps) +
Ch4 * log(testpredict_df$Ch4+.Machine$double.eps))
return(result)
}
# Calculate logloss
loss <- logloss(testSet, as.data.frame(pred))
loss #best result: 1.145054
# Read test csv
getNum <- read.csv("./test1_preprocessed.csv")
# Remove unnecessary columns
test <- subset(getNum, select = -c(No, Case, CC4,GN4,NS4,BU4,FA4,LD4,BZ4,FC4,FP4,RP4,PP4,KA4,SC4,TS4,NV4,MA4,LB4,AF4,HU4,Price4))
# Test prediction
final_predict <- predict(model, test, type="prob")
# Change col names to Ch1, Ch2, Ch3, Ch4 to fit submission format
colnames(final_predict) <- c("Ch1","Ch2","Ch3","Ch4")
# Change to dataframe
final_predict_df <- as.data.frame(final_predict)
# Add No column
final_predict_df$No <- getNum$No
# Change column orientation to fit submission format
final_predict_df <- final_predict_df[c("No","Ch1","Ch2","Ch3","Ch4")]
# Export
write.csv(final_predict_df, file = "../Output/RandomForest_best.csv", row.names = FALSE)
shiny::runApp('C:/Users/Joash Tan/Desktop/RShiny_FlowerPowerGame/Project')
runApp('C:/Users/Joash Tan/Desktop/RShiny_FlowerPowerGame/Project')
